{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import os, sys\n",
    "import datetime, time\n",
    "import gc, operator \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from config import SEED, PARTIAL_TRAIN, TEST_SIZE, NUM_LABELS\n",
    "from config import MAX_SEQUENCE_LENGTH, NUM_EPOCH, LEARNING_RATE, BATCH_SIZE\n",
    "from config import ACCUMULATION_STEPS, INPUT_DIR, WORK_DIR, TOXICITY_COLUMN, DATA_DIR\n",
    "from config import BERT_MODEL_NAME, FINE_TUNED_MODEL_PATH\n",
    "\n",
    "from utils import set_seed, convert_lines_onfly, preprocess\n",
    "from utils import calculate_overall_auc, compute_bias_metrics_for_model, get_final_metric\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 902437 records\n",
      "CPU times: user 26.7 s, sys: 1.86 s, total: 28.5 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "train_df = train_df.sample(frac=PARTIAL_TRAIN, random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "\n",
    "test_df = train_df.tail(TEST_SIZE)\n",
    "train_df = train_df.head(((train_df.shape[0]-TEST_SIZE)//BATCH_SIZE)*BATCH_SIZE)\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "sentences = preprocess(train_df['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")).values \n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "y_columns = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'] + identity_columns\n",
    "identity_sp = [ 'homosexual_gay_or_lesbian','muslim', 'black', 'white']\n",
    "\n",
    "# Convert taget and identity columns to booleans\n",
    "train_df = train_df.drop(['comment_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(train):\n",
    "    has_identity = torch.sigmoid(10*(torch.tensor((train[identity_columns].fillna(0).max(axis=1)).values)-0.4))\n",
    "    has_target = torch.sigmoid(10*(torch.tensor(train['target'].values)-0.4))\n",
    "    weights = (torch.ones(train.shape[0],dtype=torch.float64)+has_identity+has_identity*(1-has_target)+has_target*(1-has_identity)) / 4\n",
    "    weights = weights.to(dtype=torch.float32)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure every batch has similar sentences length, and shuffle the batchs\n",
    "sort_idx = np.argsort(np.array([len(x.split()) for x in sentences])).reshape(train_df.shape[0]//BATCH_SIZE,BATCH_SIZE)\n",
    "np.random.shuffle(sort_idx)\n",
    "sort_idx = sort_idx.reshape(train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 79.7 ms, total: 200 ms\n",
      "Wall time: 198 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = sentences[sort_idx]\n",
    "X = sentences                #[train_df.idx]\n",
    "y = train_df[y_columns].values[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = calculate_weights(train_df)[sort_idx].repeat(len(y_columns),1).transpose(0,1)\n",
    "weights_tensor[:,0] = weights_tensor[:,0] * (len(y_columns))/4\n",
    "weights_tensor[:,6:] = weights_tensor[:,6:] * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(torch.arange(len(X)), torch.tensor((np.abs(2.0*y-1.0)**0.5*np.sign(y-0.5)+1)/2,dtype=torch.float), weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "model.zero_grad()\n",
    "_ = model.cuda()\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "avg_val_loss = 0.\n",
    "avg_val_accuracy = 0.\n",
    "num_train_optimization_steps = int(NUM_EPOCH*len(train)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7415dfff9bbb4f828b9ca43d15fad692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6b902b847f451dbbfaeb6d76b152e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = model.train()\n",
    "tq = tqdm(range(NUM_EPOCH))\n",
    "for epoch in tq:\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0.\n",
    "    lossf = None\n",
    "    optimizer.zero_grad()\n",
    "    tk0 = tqdm(train_loader,leave = False)\n",
    "    for i , (ind_batch, y_batch, w_batch) in enumerate(tk0):\n",
    "        ind_batch.requires_grad = False\n",
    "        x_batch=torch.tensor(convert_lines_onfly(X[ind_batch.numpy()], MAX_SEQUENCE_LENGTH, tokenizer))\n",
    "        y_pred = model(x_batch.to(device)).logits\n",
    "        loss =  F.binary_cross_entropy_with_logits(y_pred, y_batch.to(device), weight=w_batch.to(device)) / ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        if (i+1) % ACCUMULATION_STEPS == 0:             # Wait for several backward steps\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if lossf:\n",
    "            lossf = 0.98*lossf+0.02*loss.item()*ACCUMULATION_STEPS\n",
    "        else:\n",
    "            lossf = loss.item()\n",
    "        tk0.set_postfix(loss = lossf)\n",
    "        avg_loss += loss.item()*ACCUMULATION_STEPS / len(train_loader)\n",
    "        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "    tq.set_postfix(avg_val_loss=avg_val_loss,avg_val_accuracy=avg_val_accuracy,avg_loss=avg_loss,avg_accuracy=avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), FINE_TUNED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation\n",
    "# The following 2 lines are not needed but show how to download the model for prediction\n",
    "sentences = preprocess(test_df['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")).values\n",
    "test_df=test_df.fillna(0)\n",
    "sort_idx=np.flip(np.argsort(np.array([len(x.split()) for x in sentences])))\n",
    "org_idx=np.argsort(sort_idx)\n",
    "X = sentences[sort_idx]\n",
    "test_preds = torch.zeros((len(X)))\n",
    "x_test = torch.arange(len(X))\n",
    "test = torch.utils.data.TensorDataset(x_test)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.load_state_dict(torch.load(output_model_file))\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "tk0 = tqdm(test_loader,leave=False)\n",
    "tranct = 0\n",
    "for i, (ind_batch,) in enumerate(tk0):\n",
    "    x_batch=torch.tensor(convert_lines_onfly(X[ind_batch.numpy()], MAX_SEQUENCE_LENGTH, tokenizer))\n",
    "    y_pred = model(x_batch.to(device)).logits\n",
    "    test_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = test_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE]+torch.sigmoid(y_pred[:, 0].cpu())\n",
    "    tranct = tranct + BATCH_SIZE * (x_batch.shape[1] == MAX_SEQUENCE_LENGTH)\n",
    "    tk0.set_postfix(trunct=tranct,gpu_memory=torch.cuda.memory_allocated() // 1024 ** 2,batch_len=x_batch.shape[1])\n",
    "    \n",
    "MODEL_NAME = 'model1'\n",
    "test_df[MODEL_NAME]=torch.sigmoid(torch.tensor(test_preds[org_idx])).numpy()\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
